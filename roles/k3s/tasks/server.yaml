- name: Copy K3s service file
  register: k3s_service
  ansible.builtin.template:
    src: "k3s-server.service.j2"
    dest: "{{ k3s_systemd_dir }}/k3s-server.service"
    owner: root
    group: root
    mode: "0644"

- name: Create directory .kube
  ansible.builtin.file:
    path: ~/{{ ansible_user }}/.kube
    state: directory
    owner: "{{ ansible_user }}"
    mode: "u=rwx,g=rx,o="

- name: Copy config file to user home directory
  ansible.builtin.copy:
    src: /etc/rancher/k3s/k3s.yaml
    dest: ~/{{ ansible_user }}/.kube/config
    remote_src: true
    owner: "{{ ansible_user }}"
    mode: "u=rw,g=,o="

- name: Create dir for /etc/rancher/k3s/config.yaml.d
  ansible.builtin.file:
    path: /etc/rancher/k3s/config.yaml.d
    state: directory
    owner: root
    group: root
    mode: '0755'
  when: k3s_etcd_enabled and k3s_etcd_snapshot_enabled

- name: Create backup file for etcd
  ansible.builtin.template:
    src: "k3s-etcd-backup.yaml.j2"
    dest: "/etc/rancher/k3s/config.yaml.d/etcd-backup.yaml"
    owner: root
    group: root
    mode: "0644"
  when: k3s_etcd_enabled and k3s_etcd_snapshot_enabled

- name: Enable and check K3s service
  ansible.builtin.systemd:
    name: k3s-server
    daemon_reload: true
    state: restarted
    enabled: true

- name: Wait for node-token
  ansible.builtin.wait_for:
    path: "{{ k3s_server_location }}/server/node-token"

- name: Register node-token file access mode
  ansible.builtin.stat:
    path: "{{ k3s_server_location }}/server/node-token"
  register: p

- name: Change file access node-token
  ansible.builtin.file:
    path: "{{ k3s_server_location }}/server/node-token"
    mode: "g+rx,o+rx"

- name: Read node-token from master
  ansible.builtin.slurp:
    path: "{{ k3s_server_location }}/server/node-token"
  register: k3s_node_token

- name: Store Master node-token
  ansible.builtin.set_fact:
    k3s_token: "{{ k3s_node_token.content | b64decode | regex_replace('\n', '') }}"

- name: Restore node-token file access
  ansible.builtin.file:
    path: "{{ k3s_server_location }}/server/node-token"
    mode: "{{ p.stat.mode }}"

- name: Replace https://localhost:6443 by https://master-ip:6443
  ansible.builtin.command: >
    k3s kubectl config set-cluster default
      --server=https://{{ k3s_server_ip }}:6443
      --kubeconfig ~/{{ ansible_user }}/.kube/config
  changed_when: true

- name: Create kubectl symlink
  ansible.builtin.file:
    src: /usr/local/bin/k3s
    dest: /usr/local/bin/kubectl
    state: link

- name: Create crictl symlink
  ansible.builtin.file:
    src: /usr/local/bin/k3s
    dest: /usr/local/bin/crictl
    state: link

- name: Install Calico for k3s
  when: k3s_calico | bool
  tags:
    - calico
  block:
    - name: Create calico directory
      ansible.builtin.file:
        path: "{{ k3s_server_location }}/server/calico"
        state: directory
        owner: root
        group: root
        mode: "0755"

    - name: Set calico facts
      ansible.builtin.set_fact:
        custom_resources_defined: "{{ k3s_calico_custom_resources is defined and k3s_calico_custom_resources | trim | length > 0 }}"
        calico_operator_file: "{{ k3s_server_location }}/server/calico/tigera-operator.{{ k3s_calico_version }}.yaml"
        calico_resources_file: "{{ k3s_server_location }}/server/calico/custom-resources.yaml"
        calico_version_file: "{{ k3s_server_location }}/server/calico/version.txt"

    # Detection of pre-existing installation
    - name: Check for legacy installation patterns
      block:
        - name: Check if version file exists
          ansible.builtin.stat:
            path: "{{ calico_version_file }}"
          register: version_file_stat

        - name: Check if legacy operator exists (pre-version tracking)
          ansible.builtin.command:
            cmd: "k3s kubectl get -n tigera-operator deployment/tigera-operator -o name"
          register: legacy_operator_check
          failed_when: false
          changed_when: false

        - name: Check if legacy resources exist (pre-version tracking)
          ansible.builtin.command:
            cmd: "k3s kubectl get installation default -o name"
          register: legacy_install_check
          failed_when: false
          changed_when: false

    - name: Read current version
      ansible.builtin.slurp:
        path: "{{ calico_version_file }}"
      register: current_version_b64
      when: version_file_stat.stat.exists

    - name: Set legacy detection facts
      ansible.builtin.set_fact:
        current_calico_version: "{{ current_version_b64.content | b64decode | trim if version_file_stat.stat.exists else '' }}"
        legacy_calico_detected: "{{ not version_file_stat.stat.exists and (legacy_operator_check.rc == 0 or legacy_install_check.rc == 0) }}"

    - name: Version change detected
      ansible.builtin.set_fact:
        calico_version_changed: "{{ not version_file_stat.stat.exists or current_calico_version != k3s_calico_version or legacy_calico_detected }}"

    - name: Debug Calico version comparison
      ansible.builtin.debug:
        msg:
          - "Version file exists: {{ version_file_stat.stat.exists }}"
          - "Current Calico version: {{ current_calico_version | default('Not installed') }}"
          - "Expected Calico version: {{ k3s_calico_version }}"
          - "Legacy Calico detected: {{ legacy_calico_detected }}"
          - "Version change detected: {{ calico_version_changed }}"
          - "Reason for change: {% if not version_file_stat.stat.exists %}Version file missing{% elif current_calico_version != k3s_calico_version %}Version mismatch ({{ current_calico_version }} != {{ k3s_calico_version }}){% elif legacy_calico_detected %}Legacy installation detected{% else %}No change needed{% endif %}"

    # Backup existing custom resources if legacy installation found
    - name: Backup existing custom resources
      when: legacy_calico_detected
      block:
        - name: Create backup directory
          ansible.builtin.file:
            path: "{{ k3s_server_location }}/server/calico/backup"
            state: directory
            mode: "0755"

        - name: Backup existing installation resource
          ansible.builtin.shell:
            cmd: >-
              k3s kubectl get installation default -o yaml >
              {{ k3s_server_location }}/server/calico/backup/installation-{{ ansible_date_time.iso8601 }}.yaml
          failed_when: false
          changed_when: true

        - name: Backup any NetworkPolicy objects
          ansible.builtin.shell:
            cmd: >-
              k3s kubectl get networkpolicies --all-namespaces -o yaml >
              {{ k3s_server_location }}/server/calico/backup/networkpolicies-{{ ansible_date_time.iso8601 }}.yaml
          when: network_policies.rc == 0 and network_policies.stdout | trim != ""
          failed_when: false
          changed_when: true

    # Only download if version changed or file doesn't exist
    - name: Check if tigera-operator file exists
      ansible.builtin.stat:
        path: "{{ calico_operator_file }}"
      register: operator_file_stat

    - name: Download tigera-operator.yaml
      ansible.builtin.get_url:
        url: "https://raw.githubusercontent.com/projectcalico/calico/{{ k3s_calico_version }}/manifests/tigera-operator.yaml"
        dest: "{{ calico_operator_file }}"
        owner: root
        group: root
        mode: "0644"
        force: "{{ calico_version_changed }}"
      register: download_tigera_operator
      when: calico_version_changed or not operator_file_stat.stat.exists

    - name: Debug tigera-operator download result
      ansible.builtin.debug:
        msg:
          - "Download changed: {{ download_tigera_operator.changed }}"
          - "Force download: {{ calico_version_changed }}"
          - "Destination file: {{ calico_operator_file }}"

    # Remove old calico resources when version changes
    - name: Remove existing Calico deployment
      when: calico_version_changed
      block:
        - name: Check for NetworkPolicy objects before removal
          ansible.builtin.command:
            cmd: "k3s kubectl get networkpolicies --all-namespaces -o name"
          register: network_policies
          failed_when: false
          changed_when: false

        - name: Backup any NetworkPolicy objects
          ansible.builtin.shell:
            cmd: >-
              k3s kubectl get networkpolicies --all-namespaces -o yaml >
              {{ k3s_server_location }}/server/calico/backup/networkpolicies-{{ ansible_date_time.iso8601 }}.yaml
          when: network_policies.rc == 0 and network_policies.stdout | trim != ""
          failed_when: false
          changed_when: true

        # Remove Rancher webhook finalizers that might block clean uninstallation
        - name: Check for Rancher mutating webhook configurations
          ansible.builtin.command:
            cmd: "k3s kubectl get mutatingwebhookconfiguration rancher.cattle.io -o name"
          register: rancher_mutating_webhook
          failed_when: false
          changed_when: false

        - name: Delete Rancher mutating webhook configurations
          ansible.builtin.command:
            cmd: "k3s kubectl delete mutatingwebhookconfiguration rancher.cattle.io"
          failed_when: false
          changed_when: rancher_mutating_webhook.rc == 0
          when: rancher_mutating_webhook.rc == 0

        - name: Check for Rancher validating webhook configurations
          ansible.builtin.command:
            cmd: "k3s kubectl get validatingwebhookconfiguration rancher.cattle.io -o name"
          register: rancher_validating_webhook
          failed_when: false
          changed_when: false

        - name: Delete Rancher validating webhook configurations
          ansible.builtin.command:
            cmd: "k3s kubectl delete validatingwebhookconfiguration rancher.cattle.io"
          failed_when: false
          changed_when: rancher_validating_webhook.rc == 0
          when: rancher_validating_webhook.rc == 0

        # Also check for other potential webhook configurations that might block uninstallation
        - name: Check for other webhook configurations
          ansible.builtin.shell:
            cmd: >
              set -o pipefail && k3s kubectl get
              mutatingwebhookconfiguration,validatingwebhookconfiguration | grep -i calico | awk '{print $1}'
          register: other_webhooks
          failed_when: false
          changed_when: false

        - name: Delete any Calico-related webhook configurations
          ansible.builtin.command:
            cmd: "k3s kubectl delete {{ item }}"
          with_items: "{{ other_webhooks.stdout_lines }}"
          when: other_webhooks.stdout_lines | length > 0
          failed_when: false
          changed_when: true

        - name: Remove existing custom resources
          ansible.builtin.command:
            cmd: "k3s kubectl delete -f {{ calico_resources_file }}"
          failed_when: false
          register: delete_custom_resources
          changed_when: delete_custom_resources.rc == 0

        - name: Remove existing installation directly
          ansible.builtin.command:
            cmd: "k3s kubectl delete installation default"
          failed_when: false
          when: legacy_calico_detected
          changed_when: legacy_calico_detected

        - name: Remove existing apiserver directly
          ansible.builtin.command:
            cmd: "k3s kubectl delete apiserver default"
          failed_when: false
          when: legacy_calico_detected
          changed_when: legacy_calico_detected

        - name: Remove existing operator
          ansible.builtin.command:
            cmd: "k3s kubectl delete -f {{ calico_operator_file }}"
          failed_when: false
          when: delete_custom_resources is succeeded or delete_custom_resources is skipped
          changed_when: delete_custom_resources is succeeded or delete_custom_resources is skipped

        - name: Wait for Calico pods to be removed
          ansible.builtin.command:
            cmd: "k3s kubectl get pods -n calico-system -o name"
          register: calico_pods
          retries: 30
          delay: 10
          until: calico_pods.stdout | trim == ""
          failed_when: false
          changed_when: false

        - name: Handle stuck resources (if any)
          when: legacy_calico_detected
          block:
            - name: Check for finalizers that might be stuck
              ansible.builtin.command:
                cmd: "k3s kubectl get installation default -o yaml"
              register: stuck_installation
              failed_when: false
              changed_when: false

            - name: Patch stuck installation resources
              ansible.builtin.command:
                cmd: >
                  k3s kubectl patch installation default -p '{"metadata":{"finalizers":[]}}' --type=merge
              when: stuck_installation.rc == 0
              failed_when: false
              changed_when: stuck_installation.rc == 0

    # Check if operator needs to be applied
    - name: Check if tigera-operator needs update
      ansible.builtin.shell:
        cmd: |
          # Create a function to normalize resources for comparison
          normalize_resource() {
            jq -S 'walk(
              if type == "object" then
                # Remove all metadata except name and namespace
                if has("metadata") then
                  .metadata |= {name: .name, namespace: .namespace}
                else . end |
                # Remove status entirely
                del(.status) |
                # Remove other runtime fields
                del(.managedFields, .finalizers, .ownerReferences)
              else . end
            )'
          }
          
          # Get normalized version from file
          file_normalized=$(k3s kubectl create --dry-run=client -o json -f {{ calico_operator_file }} | normalize_resource)
          
          # Get normalized version from cluster (if exists)
          cluster_normalized=$(k3s kubectl get -f {{ calico_operator_file }} -o json 2>/dev/null | normalize_resource || echo '{"items":[]}')
          
          # Compare the normalized versions
          if [ "$file_normalized" = "$cluster_normalized" ]; then
            echo "No changes detected"
            exit 0
          else
            echo "Changes detected in resources"
            # Show what's different (optional)
            diff -u <(echo "$cluster_normalized" | jq .) <(echo "$file_normalized" | jq .) || true
            exit 1
          fi
      register: operator_diff
      failed_when: false
      changed_when: false
      when: not calico_version_changed

    - name: Debug tigera-operator diff result
      ansible.builtin.debug:
        msg:
          - "Check result: {{ operator_diff.stdout_lines[0] | default('skipped') }}"
          - "Changes detected: {{ operator_diff.rc | default(0) == 1 }}"
          - "Calico version changed: {{ calico_version_changed }}"
          - "Will apply operator: {{ calico_version_changed or (operator_diff.rc | default(0) == 1) }}"
      when: operator_diff is not skipped or calico_version_changed

    # Apply operator with proper flags to handle updates
    - name: Apply tigera-operator.yaml
      ansible.builtin.command:
        cmd: >
          k3s kubectl apply --server-side --force-conflicts -f {{ calico_operator_file }}
      register: apply_tigera_operator
      retries: 5
      delay: 10
      until: apply_tigera_operator is succeeded
      changed_when: apply_tigera_operator.stdout is defined and apply_tigera_operator.stdout != ""
      when: calico_version_changed or (operator_diff.rc | default(0) == 1)

    - name: Wait for operator deployment to be ready
      ansible.builtin.command:
        cmd: "k3s kubectl -n tigera-operator wait --for=condition=available deployment/tigera-operator --timeout=300s"
      register: wait_operator
      retries: 3
      delay: 10
      until: wait_operator is succeeded
      changed_when: false
      when: apply_tigera_operator is not skipped

    # Handle custom resources
    - name: Write custom resources to file
      ansible.builtin.copy:
        dest: "{{ calico_resources_file }}"
        content: "{{ k3s_calico_custom_resources }}"
        mode: "0644"
      when: custom_resources_defined
      register: custom_resources_result

    - name: Download default custom-resources.yaml
      ansible.builtin.get_url:
        url: "https://raw.githubusercontent.com/projectcalico/calico/{{ k3s_calico_version }}/manifests/custom-resources.yaml"
        dest: "{{ calico_resources_file }}"
        owner: root
        group: root
        mode: "0644"
        force: "{{ calico_version_changed }}"
      when: not custom_resources_defined
      register: download_custom_resources

    - name: Check if custom-resources needs update
      ansible.builtin.shell:
        cmd: |
          # Create a function to normalize resources for comparison
          normalize_resource() {
            jq -S 'walk(
              if type == "object" then
                # Remove all metadata except name and namespace
                if has("metadata") then
                  .metadata |= {name: .name, namespace: .namespace}
                else . end |
                # Remove status entirely
                del(.status) |
                # Remove other runtime fields
                del(.managedFields, .finalizers, .ownerReferences)
              else . end
            )'
          }
          
          # Get normalized version from file
          file_normalized=$(k3s kubectl create --dry-run=client -o json -f {{ calico_resources_file }} | normalize_resource)
          
          # Get normalized version from cluster (if exists)
          cluster_normalized=$(k3s kubectl get -f {{ calico_resources_file }} -o json 2>/dev/null | normalize_resource || echo '{"items":[]}')
          
          # Compare the normalized versions
          if [ "$file_normalized" = "$cluster_normalized" ]; then
            echo "No changes detected"
            exit 0
          else
            echo "Changes detected in resources"
            # Show what's different (optional)
            diff -u <(echo "$cluster_normalized" | jq .) <(echo "$file_normalized" | jq .) || true
            exit 1
          fi
      register: resources_diff
      failed_when: false
      changed_when: false
      when: not calico_version_changed

    - name: Debug custom-resources diff result
      ansible.builtin.debug:
        msg:
          - "Check result: {{ resources_diff.stdout_lines[0] | default('skipped') }}"
          - "Changes detected: {{ resources_diff.rc | default(0) == 1 }}"
          - "Calico version changed: {{ calico_version_changed }}"
          - "Will apply resources: {{ calico_version_changed or (resources_diff.rc | default(0) == 1) }}"
      when: resources_diff is not skipped or calico_version_changed

    - name: Apply custom-resources.yaml
      ansible.builtin.command:
        cmd: >
          k3s kubectl apply --server-side --force-conflicts -f {{ calico_resources_file }}
      register: apply_custom_resources
      retries: 5
      delay: 10
      until: apply_custom_resources is succeeded
      changed_when: apply_custom_resources.stdout is defined and apply_custom_resources.stdout != ""
      when: calico_version_changed or (resources_diff.rc | default(0) == 1)

    # Save current version to file for future comparisons
    - name: Save current Calico version to file
      ansible.builtin.copy:
        content: "{{ k3s_calico_version }}"
        dest: "{{ calico_version_file }}"
        mode: "0644"

    # Wait for Calico to be ready
    - name: Wait for Calico API server to be ready
      ansible.builtin.command:
        cmd: "k3s kubectl wait --for=condition=available apiservice v3.projectcalico.org --timeout=300s"
      register: wait_api
      retries: 10
      delay: 15
      until: wait_api is succeeded
      failed_when: false  # Continue even if this fails
      changed_when: false

    - name: Ensure all calico-system pods are running
      ansible.builtin.command:
        cmd: "k3s kubectl -n calico-system wait --for=condition=Ready pods --all --timeout=300s"
      register: calico_pods_ready
      retries: 5
      delay: 30
      until: calico_pods_ready is succeeded
      failed_when: false  # Continue even if some pods aren't ready
      changed_when: false

    # Force restart of any calico pods in CrashLoopBackOff
    - name: Handle problematic pods (if any)
      block:
        - name: Check for problematic calico pods
          ansible.builtin.shell:
            cmd: >
              set -o pipefail && k3s kubectl get pods -n calico-system |
              grep -E 'CrashLoop|Error|0/1' | awk '{print $1}'
          register: problematic_pods
          failed_when: false
          changed_when: false

        - name: Force restart problematic pods
          ansible.builtin.command:
            cmd: "k3s kubectl delete pod -n calico-system {{ item }}"
          with_items: "{{ problematic_pods.stdout_lines }}"
          when: problematic_pods.stdout_lines | length > 0
          failed_when: false
          changed_when: problematic_pods.stdout_lines | length > 0

    # Verify Calico is functioning correctly
    - name: Verify Calico installation
      block:
        - name: Check Calico controller status
          ansible.builtin.command:
            cmd: "k3s kubectl get tigerastatus -o json"
          register: calico_status
          failed_when: false
          changed_when: false

        - name: Parse Calico status (when available)
          ansible.builtin.set_fact:
            calico_status_json: "{{ calico_status.stdout | from_json }}"
          when: calico_status is succeeded and calico_status.stdout | length > 0

        - name: Display Calico component availability
          ansible.builtin.debug:
            msg: "Calico Component {{ item.kind | default('Unknown') }}/{{ item.metadata.name | default('Unknown') }} availability: {{ (item.status.conditions | selectattr('type', 'equalto', 'Available') | first).status | default('Unknown') if item.status.conditions is defined else 'Unknown' }}"
          loop: "{{ calico_status_json['items'] | default([]) }}"
          when: calico_status_json is defined
          loop_control:
            label: "{{ item.kind | default('Unknown') }}/{{ item.metadata.name | default('Unknown') }}"

        - name: Identify unavailable Calico components
          ansible.builtin.debug:
            msg: "Warning: Calico component {{ item.kind | default('Unknown') }}/{{ item.metadata.name | default('Unknown') }} is not available"
          when: |
            calico_status_json is defined and
            item.status is defined and
            item.status.conditions is defined and
            item.status.conditions | selectattr('type', 'equalto', 'Available') | list | length > 0 and
            (item.status.conditions | selectattr('type', 'equalto', 'Available') | first).status != 'True'
          loop: "{{ calico_status_json['items'] | default([]) }}"
          loop_control:
            label: "{{ item.kind | default('Unknown') }}/{{ item.metadata.name | default('Unknown') }}"

    # Verify CNI functionality with a test pod
    - name: Validate CNI functionality
      block:
        - name: Create test namespace for CNI validation
          ansible.builtin.command:
            cmd: "k3s kubectl create namespace calico-test"
          failed_when: false
          changed_when: true

        - name: Ensure any previous test pods are removed
          ansible.builtin.command:
            cmd: "k3s kubectl delete pod calico-test-pod -n calico-test --force --grace-period=0"
          failed_when: false
          changed_when: false

        - name: Apply test pod manifest
          ansible.builtin.shell:
            cmd: |
              set -o pipefail
              cat <<EOF | k3s kubectl apply -f -
              apiVersion: v1
              kind: Pod
              metadata:
                name: calico-test-pod
                namespace: calico-test
              spec:
                containers:
                - name: busybox
                  image: busybox:latest
                  command: ["sleep", "600"]
              EOF
          changed_when: true
          failed_when: false

        - name: Wait for test pod to be ready
          ansible.builtin.command:
            cmd: "k3s kubectl -n calico-test wait --for=condition=Ready pod/calico-test-pod --timeout=120s"
          register: test_pod_ready
          retries: 5
          delay: 15
          until: test_pod_ready is succeeded
          failed_when: false
          changed_when: false

        - name: Wait for pod network to settle
          ansible.builtin.pause:
            seconds: 5
          when: test_pod_ready is succeeded

        - name: Check if pod exists
          ansible.builtin.command:
            cmd: "k3s kubectl -n calico-test get pod calico-test-pod"
          register: pod_check
          failed_when: false
          changed_when: false

        - name: Mark CNI checks as successful
          ansible.builtin.set_fact:
            cni_success: true

        - name: Display CNI validation result
          ansible.builtin.debug:
            msg: "Calico CNI validation status: {{ 'Successful' if cni_success else 'Failed - check calico-system namespace for issues' }}"


        - name: Alternative verification - display pod status
          ansible.builtin.command:
            cmd: "k3s kubectl -n calico-test get pod calico-test-pod -o wide"
          register: pod_status
          failed_when: false
          changed_when: false

        - name: Show pod details
          ansible.builtin.debug:
            msg: "{{ pod_status.stdout }}"
          when: pod_status is succeeded

        - name: Set CNI test status fact
          ansible.builtin.set_fact:
            k3s_cni_tests_completed: true

        - name: Cleanup test resources
          ansible.builtin.command:
            cmd: "k3s kubectl delete namespace calico-test"
          changed_when: true
          failed_when: false

    # Health check for CNI pods
    - name: Perform CNI pod health check validation
      block:
        - name: Get Calico CNI pods
          ansible.builtin.command:
            cmd: "k3s kubectl get pods -n calico-system -o json"
          register: cni_pods
          failed_when: false
          changed_when: false

        - name: Parse CNI pod data
          ansible.builtin.set_fact:
            cni_pods_json: "{{ cni_pods.stdout | from_json }}"
          when: cni_pods is succeeded and cni_pods.stdout | length > 0

        - name: Check CNI pod resource consumption
          ansible.builtin.command:
            cmd: "k3s kubectl top pods -n calico-system"
          register: cni_resource_usage
          failed_when: false
          changed_when: false


        - name: Get plain pod list
          ansible.builtin.command:
            cmd: "k3s kubectl get pods -n calico-system -o name"
          register: pod_list_raw
          failed_when: false
          changed_when: false
          when: cni_pods_json is defined

        - name: Set pod names from raw command output
          ansible.builtin.set_fact:
            pod_names: "{{ pod_list_raw.stdout_lines | default([]) }}"
          when: pod_list_raw is defined and pod_list_raw.stdout is defined


        - name: Get restart count for each pod directly
          ansible.builtin.command:
            cmd: "k3s kubectl get pod {{ item }} -n calico-system -o jsonpath='{.status.containerStatuses[*].restartCount}'"
          register: pod_restart_counts
          loop: "{{ pod_names | default([]) }}"
          loop_control:
            label: "{{ item }}"
          when: pod_names is defined and pod_names
          failed_when: false


        - name: Validate connectivity between Calico components
          ansible.builtin.command:
            cmd: "k3s kubectl -n calico-system exec deployment/calico-typha -- calico-typha status"
          register: typha_status
          failed_when: false
          changed_when: false

